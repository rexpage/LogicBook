\chapter[Computer Systems: Simple Principles Lead to Complex Behavior][Computer Systems]{Computer Systems: Simple Principles, Complex Behavior}

\section{Hardware and Software}

Computer systems, both hardware and software, are some of the
most complicated artifacts that people have ever created.
Yet, computer systems are
applications of principles of logic that philosophers
have been developing for thousands of years.
The logic that arose from this long effort,
together with some engineering artifacts
that emerged from its framework,
will be the primary topics of this book.

The \index{hardware}hardware component of a computer system is made up
of physical pieces of equipment such as monitors,
keyboards, printers, web cameras, and USB drives.
Hardware also includes components inside the computer enclosure,
such as chips, cables, hard drives, and circuit boards.
The properties of hardware devices are largely fixed when the system is
constructed. For example, the capacity of a hard drive is
determined when it is manufactured. A three terabyte drive
stores three terabytes of data, not more.
A particular cable may constructed to carry twenty-five different signals
simultaneously. Need to send twenty-six signals at the same time?
Get a different cable, or more of them.

The hardware that makes up a computer system is not much different
than the electronics inside a television set or a navigational device,
but computer hardware can do something that most
consumer electronic devices cannot do.
It can respond to the software component of the computer system
to exhibit an unlimited range of behaviors.
The \index{software}software component is a collection of computer programs,
and programs can be added to the collection, or removed from it,
at any time.
So, a computer system is a multipurpose device in the extreme,
capable of doing things that its designers never thought about.

You have no doubt heard of various computer chips.
Maybe your laptop is powered by an Intel i7 or
an AMD Athlon X4.
Software for these chips consists of a list of
instructions that, when carried out one by one, in sequence,
modify the state of the computer system to make it perform
the function the software designer had in mind.
However, software designers rarely put together programs using
the \index{instruction set}instruction set of the computer chip
that performs the computation.
Instead, software designers use a more compact notation to
specify the desired behavior, and another piece of software
builds the list of chip instructions from those specifications.

The instructions that the chip can carry out
are almost always different from the instructions
that the software designer uses to build computer programs.
There are hundreds of different programming languages
that software designers can choose from.
In most cases software expressed in these languages consists
of a list of instructions to be carried out, one by one,
in the manner of chip instructions,
but each instruction in the programming language gives
rise to a sequence of at least a few and sometimes many
commands in the instruction set of the underlying computer chip.

However, software as a list of instructions is
not the only alternative.
Another alternative uses drawings of
system configurations to build software.
For example, scientists use
\index{LabView}LabView
to control laboratory equipment.
Engineers use it to design digital systems,
and college students use it in projects to
design electronic devices.
Computer programs in LabView look like engineering drawings,
not like lists of instructions.
Another visual language,
Scratch, created by the MIT Media Lab,
is used by children to construct software for games
and other educational projects.
LabView and Scratch programs don't
look anything like chip instructions,
but they are just as capable of describing computations.

A related alternative is to specify software as
a collection of equations in which
the left-hand side names a computation to be carried out
and the right-hand side reduces the computation to
some smaller components producing parts of the result
and combining them to deliver the whole.
An advantage of the equation-based approach
is that a form of reasoning similar to that used
in ordinary, high-school algebra can be used to derive
guarantees that the software produces results consistent
with design requirements.
Another advantage is that the equations, themselves,
are often inspired directly by design requirements,
so that if the software designer can think of tests
that the software should pass, the software itself
can emerge from specifications of the tests.

Instruction lists, visual programming languages, and equation-based
software are fundamentally different, but they
are fully equivalent in terms of capabilities.
Since the primary focus of this text is on logical reasoning
about properties of hardware and software components of computer systems,
we will use the equation-based approach,
but specification of the hardware and software artifacts that we study
is not limited to that domain.

\begin{aside}
Logicians and mathematicians have been studying
\index{computation models}\index{model!of computation}models of computation
since before computers were invented. This was done, in part, to
answer a deep mathematical question: What parts of mathematics can,
in principle, be fully automated?  Is it possible to
build a machine that can prove all mathematical truths?

Many different models of computation were developed to study this question:
\index{Turing machine}Turing machines, \index{lambda calculus}lambda calculus,
partial recursive functions,
unrestricted grammars, Post production rules, random-access machines,
and many others.
Historically, Turing machines have provided the canonical foundation for computation,
but the random-access model conforms more closely to modern computers,
and the lambda calculus model is used in both computer science theory and practice.
The equation-based model of computation that is the focus of this book
falls into the lambda calculus bailiwick.

It is truly remarkable that all of these models of
computation are equivalent.  That is,
any computation that can be described in one of the models can
be described in any of the others.
A conjecture based on this equivalence
and known as the Church-Turing thesis
says that all realizable computations can
be carried out by a Turing machine or by
any equivalent model of computation, such as
the lambda-calculus model.
Much of computer science theory
revolves around this hypothesis.

Another remarkable fact is that some problems cannot be solved
by any computer program.
Alan \index{Turing, Alan}Turing, one of the first theoretical computer scientists,
was the first to describe
\index{uncomputable}\seeonlyindex{incomputable}{uncomputable}uncomputable problems,
and soon afterwards the logician Kurt \index{G\"odel, Kurt}G\"odel
showed that mathematical systems of reasoning
are \index{incompleteness}incomplete. No formal system can be used
to prove all mathematical truths.
The theorems of Turing and G\"odel on uncomputability and incompleteness
prove that it is not possible to build a machine that can verify all mathematical
truths, not even in principle.

\caption{Models of Computation}
\label{aside-model-of-computation}
\end{aside}

It is software that gives a computer system its power and flexibility.
An iPhone, for example, has a screen that
can display millions of pixels,
but it is the software that determines whether the
iPhone displays an album cover or a weather update.
Software makes hardware useful by extending its range of behavior.
For instance, iPhone audio hardware may be able to produce
only a single tone at a time, but the software controlling it
can produce a sequence of tones
that sound like Beethoven's Fifth Symphony.

You can think of the hardware as
the parts in a computer that you can see,
and the \index{software}software as information that tells the computer what to do.
However, the distinction between hardware and software is not as
clear cut as this suggests. Many hardware components actually
encode software directly and control other pieces of the system.
In fact, many techniques used in the design of
hardware were originated to build software.
Major elements of hardware designs look like software
and can use the same language and notation.
A major theme of this book
is that both hardware and software are realizations of formal logic.
In this sense, computer systems are logic in action.

\section{Structure of a Program}

The distinction between hardware and software
leaves many questions to the imagination.
\begin{quote}
\begin{enumerate}
\item How can software control hardware?\\
      Example: Instruct an audio device to emit a sound.
\item How can software detect the status of hardware?\\
      Example: Determine whether a switch is pressed or not.
\item What kinds of instructions can software give to hardware?\\
      Examples: Add numbers. Replace one formula by another. Select a formula.
\end{enumerate}
\end{quote}

A model of computation (Aside~\ref{aside-model-of-computation})
shows how software can control hardware,
so there are as many answers to the first of these questions
as there are models of computation, and
logicians have been very prolific when it
comes to constructing such models.
Luckily, since there are many equivalent models,
the requirements of a project can guide the choice of models.
Generally, a programming language based on any model of computation
includes primitive arithmetic and logic operators
(addition, multiplication, etc)
and provides the ability to define new operators
using either intrinsic primitive operators or previously defined operators.

\begin{aside}
We will use the terms ``operator'' and ``function'' interchangeably.
Some treatments use these terms to mean different things,
but we find it more convenient in this presentation
to think of them as the same thing.
So, when we say \index{function}function we mean \index{operator}operator and vice versa.
What we're talking about when we use either term
is a transformation that delivers results when supplied with input.
We refer to the results delivered by an operator as its ``value'',
and we refer to the input supplied to the operator as its \index{operand}operands.
Sometimes we use the term \index{argument}``argument''
or \index{parameter}``parameter'' instead of ``operand,''
usually in connection with the term ``function,''
but the terms carry the same meaning in any case.

To summarize, an operator (aka, function) is supplied with operands
(aka, parameters or arguments) and delivers a value.
A formula like $x + y$ denotes the value delivered
by the addition operator when supplied with the operands $x$ and $y$.
More generally, an operator $f$, when supplied with operands $x$ and $y$,
produces a value, usually denoted algebraically as $f(x,y)$.
If $f$ were the arithmetic addition operator ($+$),
then $f(x,y)$ would stand for the value $x+y$, $f(2,2)$ would denote 4,
$f(3,7)$ would stand for 10, and $f(2x,5)$ would mean $2x+5$.

\caption{Operators, Operands, Functions, Parameters, Arguments}
\label{operations-and-functions}
\end{aside}

Once we take the familiar operators
and the ability to define new ones
as basic elements of a computation model,
we can talk about what it is possible for software to do.
Software can affect
hardware by the values that an operator delivers.
For example, a computer program can tell an iPhone what to display
on its screen by delivering a matrix of pixels.
Each entry in the matrix
can be a number that represents a color: 16,777,215 for
red or 65,280 for green, for example.
Similarly, the hardware can inform the
software of the status of a component by
invoking an operator and supplying the status as an operand.
For example, touching the screen of an iPhone might
trigger a software operator and supply the operator with
the coordinates of the pixel selected by the touch.
The operator could then control the device's response.
Other gestures, such as tapping or
scrolling, would trigger different operators.

To be more specific, let's consider a program to control
a device that plays the game of \index{rock-paper-scissors}\index{game, rock-paper-scissors}rock-paper-scissors against a human opponent.
The device has three buttons, allowing the human player to select rock, paper, or
scissors.  It also has a display unit.
When the human player presses a button,
the display unit shows the devices's choice (rock, paper, or scissors)
and shows the winner of the round.
The program will compute the device's choice
by invoking an operator called \texttt{emily},\footnote{This operator is
named after a daughter of one of the authors who, when she was young,
played the rock-paper-scissors game just like the program described here.}
This operator will deliver
rock or paper or scissors as its value.
We could use numbers like 0, 1, and 2 as shorthand for these values,
but most programming languages include many
kinds of information, not just numbers,
so we are going to stick with the longer names
to make it easier for us to keep track of what things mean.
Another part of the software will compare the human player's
selection with that of the device and determine the winner of the round.

To make the game fair, the program will use
separate operators for the device's choice and
for examining the choices to determine the winner.
That way, the device cannot know the human player's choice before making its own.
The operator \texttt{emily} will deliver the device's choice for a round of the game.
It needs some kind of information because an operator with no operands
always delivers the same value, and that would make for a very boring game.
Any player will have seen the choices of the other player for
all previous rounds, so it seems fair to use some of that information in
making a choice for the next round.
To keep things simple, we will supply the human player's choice for the
immediately previous round to \texttt{emily} as an operand.
With that in mind, we could design the operator \texttt{emily} to choose
a play according to the following scheme.
\begin{displaymath}
emily(u) =
   \left\{
        \begin{array}{ll}
        \mbox{rock}     & \mbox{if } u = \mbox{scissors} \\
        \mbox{paper}    & \mbox{if } u = \mbox{rock} \\
        \mbox{scissors} & \mbox{otherwise}
        \end{array}
   \right.
\end{displaymath}

The other operator, which we will call \texttt{score},
has two operands, one specifying the choice of the device in the current round,
the other specifying the human player's choice.
The \texttt{score} operator delivers a value with two elements:
the winner of the round and the human player's choice for that round.
It reports the human player's choice so that the software
can inform  the operator \texttt{emily} of that choice.
It can then use the choice to determine the device's selection for the next round
according to the scheme we specified for \texttt{emily}.
The following scheme for the \texttt{score} operator specifies,
for each possible pair of operands,
the two-element value that the operator delivers.

\begin{displaymath}
score(c,u) =
   \left\{
        \begin{array}{ll}
        (\mbox{none}, u)     & \mbox{if } c = u \\
        (\mbox{computer}, u) & \mbox{if } (c,u) = (\mbox{rock}, \mbox{scissors}) \\
        (\mbox{human}, u)    & \mbox{if } (c,u) = (\mbox{rock}, \mbox{paper}) \\
        (\mbox{computer}, u) & \mbox{if } (c,u) = (\mbox{paper}, \mbox{rock}) \\
        (\mbox{human}, u)    & \mbox{if } (c,u) = (\mbox{paper}, \mbox{scissors}) \\
        (\mbox{computer}, u) & \mbox{if } (c,u) = (\mbox{scissors}, \mbox{paper}) \\
        (\mbox{human}, u)    & \mbox{if } (c,u) = (\mbox{scissors}, \mbox{rock})
        \end{array}
   \right.
\end{displaymath}

We could make the device play a more sophisticated game of
rock-paper-scissors by defining a different \texttt{score} operator.
For example, the operator might
keep track of the number of times that the player has selected scissors
in previous rounds, or rock or paper, or it might keep track of all three.
Since it's the software that decides what to remember
from each round, this choice can be changed easily.
Such is the flexibility of software.

The way we have described the operators \texttt{emily} and \texttt{score}
illustrate an important point about our computational model.
A program in our model consists of a collection of equations
defining mathematical functions that start each computation from scratch,
based only on their input. They cannot ``remember'' anything from previous computations.
This will come as a surprise to programmers accustomed
to other computational models such as those on which
programming languages like Java or C++ are based.
In those models, programs use variables to record values
and can use or update the recorded values later in the computation.
In our equation-based model,
programs consist of collections of mathematical functions (operators)
that base their results entirely on their operands.
Operators do not make use of recorded values
beyond the data supplied in operands.
This makes it possible to
understand equation-based programs in terms of classical
logic and traditional algebraic formulas.

After the first round of the game, the display component of the hardware
for the rock-paper-scissors game
would show the first element $w_1$ of the pair $(w_1, u_1)$ delivered by the formula
\texttt{score}(\texttt{emily}(none), $b_1$),
where $b_1$ stands for rock, paper, or scissors, depending on which button
the human player pressed.
For the next round the outcome would be
$(w_2, u_2)$ $=$ \texttt{score}(\texttt{emily}($u_1$), $b_2$),
where $b_2$ is the button pressed by the human player for the new round,
and the display would show the value of $w_2$.
A game of five rounds would correspond to a sequence of equations,
each showing the outcome of a particular round.
The display hardware would show the winner of each round,
and the software would supply the button press of the previous round, which
is delivered by the \textit{score} operator as the second
element of a pair, to the \textit{emily} operator for the next round.
\begin{displaymath}
%   \left\{
        \begin{array}{ll}
        (w_1, u_1) = \mbox{score}(\mbox{emily}(\mbox{none}), b_1) \\
        (w_2, u_2) = \mbox{score}(\mbox{emily}(u_1), b_2) \\
        (w_3, u_3) = \mbox{score}(\mbox{emily}(u_2), b_3) \\
        (w_4, u_4) = \mbox{score}(\mbox{emily}(u_3), b_4) \\
        (w_5, u_5) = \mbox{score}(\mbox{emily}(u_4), b_5)
        \end{array}
%   \right.
\end{displaymath}

A more complicated version of the software
could define an operator \textit{play} that would
generate a sequence of $n$ plays, where $n$ stands for
an operand supplied to \textit{play}.
In such a scenario, there would need to be some way to supply an operand $n$
to the operator \textit{play}, where $n$
might be a fixed number, like five or ten, or
might be a number selected with the help of another button on the hardware.
The point is that the hardware provides a fixed set of
components such as displays and buttons and that the software
specifies a way to use those components.

\section{Deep Blue and Inductive Definitions}

The equation-based model of computation is small,
but it has as much power to specify computations
as any other known model. Knowing the great diversity of things that computers can do,
it seems reasonable to expect them to be much more complicated than that.
It is true that computers have many more components than, for example,
rock-paper-scissors device, but in fundamental terms,
they have the same basic structure.
Complex behavior from modest beginnings, a bargain if there ever was one.

Consider \index{Deep Blue}Deep Blue, the
computer that beat world champion Gary Kasparov in a chess match on May 11,
1997. It was the first time a computer had performed so well in a
game that challenges the mental capacities of human players.
The Deep Blue chess-playing software can be specified as an operator whose input is
an 8x8 matrix showing the positions of the pieces
on the board after a move by the human player (Kasparov, in the 1997 game).
The operator would deliver a result showing
either the positions on the board after its move or a special
white-flag token used to resign the game.

In principle, a chess-playing computer program can be an operator
in software with the following characteristics.
Given a matrix describing the board at a particular point in a game,
the operator determines all possible legal moves.  If there
are no legal moves, the operator delivers the white flag,
signaling that the Deep Blue has resigned.
If there is a legal move that results
in checkmate, the operator delivers the 8x8 matrix specifying the
new board position after that move.  Otherwise, for each legal move,
the operator considers
each of the possible moves in response by the human opponent.
Each one of those moves leads to in a new board that can be
examined by the chess-playing operator
to choose a move based on a calculation that estimates the value of
board positions.

An operator that follows this strategy can be defined
in a circular way, circular in the sense that
the operator invokes itself with new operands.
Circular definitions of this kind are common in mathematics.
We call them inductive definitions.\footnote{Operators with
\seeonlyindex{inductive definition}{definition}\index{definition!inductive (circular)}inductive definitions
are sometimes called \index{recursive}``recursive'' functions.
We try to avoid that term because it is
often associated with specialized ways to carry out the computation,
but we may slip up from time to time and call them recursive functions.
Besides, you should know the term so you'll know what people are
talking about if they mention it.}
The trick that makes inductive definitions useful in mathematics
is that at the point of circularity,
the invocation of the operator being defined
has operands that are closer to those in a non-circular part of the
definition than to the original operands.

Inductive definitions
will play a central role throughout the book
and will be discussed in great detail, but we want to provide an
introduction of sorts at this point, to get the ball rolling.
A natural number is a whole number that is not negative: $0, 1, 2, 3, \dots$.
The sum of the five \index{reciprocal}reciprocals of the
natural numbers $1$ through $5$
can be written as an algebraic formula, as in the following equation.
\begin{displaymath}
h5 = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5}
\end{displaymath}
This is non-inductive definition of $h5$,
and $h5$ has no operands, so it simply stands for a number,
namely $\frac{137}{60}$, which is about $2.3$.

We could, of course, have written a formula for the sum
of the first ten reciprocals or a sum with any number of terms.
Sums of this form comprise a mathematical entity known as
the \index{harmonic series}harmonic \index{series, harmonic}series.
They are known as the partial sums of the series,
and we can define  an operator $h$ that computes such a sum
with any specified number $n$ of terms.
\begin{displaymath}
h(n) = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \dots  \frac{1}{n}
\end{displaymath}

As a step in that direction, observe that for any natural number $n$,
the sum $h(n+1)$ of the first $n+1$ reciprocals
is the same as the sum of the first $n$ reciprocals, that is $h(n)$, plus $\frac{1}{n+1}$.
Suppose we specify that $h(0)$ stands for zero.\footnote{Our
informal definition is that $h(n)$ stands for the sum of the first $n$ reciprocals.
When $n$ is zero, there would be no terms in the sum,
so the standard convention that the sum of an empty set of numbers
is zero is consistent with the equation $h(0) = 0$.}
Then, the following equations express that decision and
the observation about the relationship between $h(n)$ and $h(n+1)$.
\index{equation, by name!\{h0\}, \{h1\}}
\label{reciprocalsdef}
%\begin{displaymath}
%h(n) =
%\left\{
%        \begin{array}{ll}
%                0                    & \mbox{if } n = 0 \\
%                h(n-1) + \frac{1}{n} & \mbox{otherwise}
%        \end{array}
%\right.
%\end{displaymath}
\begin{center}
\begin{tabular}{ll}
$h(n+1)$ $=$ $h(n) + \frac{1}{n+1}$ & \{h1\}\\
$h(0)$   $=$ $0$                    & \{h0\}\\
\end{tabular}
\end{center}

It will be a common practice in this text to
attach names to equations to make it easy to discuss them.
In this case, we have used the names \{h1\} and \{h0\}.
The equation \{h1\} is \seeonlyindex{circular definition}{definition}circular
because both sides of the equation refer to a value delivered by the operator $h$.
However, the operand $n$ in the circular reference
(that is, the formula $h(n)$ on the right-hand side of the equation \{h1\})
is closer to zero than the operand $(n+1)$ on the left-hand side of the equation.
Furthermore, in equation \{h0\}, the operand on the left-hand side is zero and
the equation is not circular.

It turns out, and we will study this is in great detail later,
that these two characteristics of a collection of equations
(a reduced operand on the right-hand side in circular equations
and a specific operand on the left-hand side in non-circular equations)
turns circular equations into a useful definition of an operator.
Therefore, we can say that equations \{h1\} and \{h0\}
define the operator $h$. The definition is mathematically rigorous
and is also computational.
To see how this works, observe that, according to equation \{h1\},
$h(5) = h(4)+\frac{1}{5}$.
Furthermore, \{h1\} also says that $h(4) = h(3)+\frac{1}{4}$.
Combining these equations algebraically leads to the equation
$h(5) = h(3)+\frac{1}{4}+\frac{1}{5}$.
Continuing the analysis in this way, using equation \{h1\} at each step,
produces the following sequence of equations.
\begin{center}
\begin{tabular}{l}
$h(5) = h(4) + \frac{1}{5}$ \\
$h(5) = h(3) + \frac{1}{4} + \frac{1}{5}$\\
$h(5) = h(2) + \frac{1}{3} + \frac{1}{4} + \frac{1}{5}$\\
$h(5) = h(1) + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5}$\\
$h(5) = h(0) + \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5}$\\
\end{tabular}
\end{center}

Equation \{h0\} says $h(0) = 0$,
so the last equation is equivalent to the following one.
\begin{center}
\begin{tabular}{l}
$h(5) = 0 + \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5}$\\
\end{tabular}
\end{center}

This equation needs no analysis, just a little arithmetic
to compute $h(5) = \frac{137}{60}$.
In this way the equations \{h1\} and \{h0\} provide not just
a definition of the operator $h$, but also a scheme for computing $h(n)$
for any natural number $n$.
Furthermore, other properties of the operator $h$,
such as the fact that the formula $h(n)$
must in all cases stand for a strictly positive number
and that its value must be a ratio of whole numbers,
can be derived from \{h1\} and \{h0\}.
It turns out that every computable function has a definition consisting
of some equations specifying properties of the function,
and this approach provides the basis for much of the reasoning
discussed in this text.
The implications of inductive equations are broader than they seem at first glance.

To get back to the chess-playing computer, the problem with our
na\"ive approach is that there are too many moves to consider.
While the function can, \emph{in principle},
select the best move given a particular arrangement of pieces on the chess board,
\emph{in practice} the computation
would take too much time.
The sun in our solar system would be long dead
before the computer could decide on its first move.

\index{Deep Blue}\index{chess}Deep Blue
did play in more-or-less this way, but it didn't look at board positions
all the way to the end of the game. Most of the time, it looked
six to eight moves ahead. Even that is a very big computation,
but it was a feasible one for Deep Blue because it had
thousands of processing units and could consider many moves at the same time.
That gave it the ability to analyze about 200 million board positions per second.
The combination of massive computational power and limited look-ahead
made it possible for Deep Blue to outplay
the most accomplished human chess player of the time.

Deep Blue is a combination of a complicated computer
and a computer program with a long, complex definition.
The Deep Blue software conformed to a conventional computation model,
not the equation-based model we used to describe it,
but in principle the program could have been designed
using equations in a manner similar to the
way equations provided a definition of the operator $h$.

\begin{ExerciseList}
\Exercise Can you use equations \{h0\} and \{h1\}
(page \pageref{reciprocalsdef})
to compute $h(-1)$?  How about $h(\frac{1}{2})$? Why not?

\Exercise Define an operator to compute \emph{factor}$(k, n)$, which is true
if $k$ is a factor of $n$ and false otherwise.
Assume that the formula $mod(k,n)$ stands for the remainder
in $k \div n$.
For example, $mod(17,5)=2$, $mod(9,2)=1$, and $mod(12,4)=0$.
Use the $mod$ function in your definition of \emph{factor}.\\
\emph{Note}: For natural numbers $k$ and $n$, $k$ is a factor $n$
if there is a number $m$ such that $n = km$.\\
\emph{Hint}: You will need only one equation.

\Exercise \label{largest-factor}
        Define an operator to compute \emph{lf}$(n)$,
        the largest factor of $n$ other than $n$ itself.
        For example, \emph{lf}$(30)=15$ and
        \emph{lf}$(15)=5$.
        Assume that the formula
        \emph{lft}$(n,k)$ stands for the largest factor of
        $n$ that doesn't exceed $k$ (the largest factor up to $k$).
        For example, \emph{lft}$(30,7)=6$.
        In your definition, use a formula like \emph{lft}$(n,k)$,
        with appropriate choices for $n$ and $k$.
        You can use the notation $\lfloor n \div k\rfloor$
        to denote the largest integer that does not exceed $n \div k$
        (that is, $n \div k$ rounded down to an integer).

\Exercise \label{prime-predicate}
        Define an operator to compute $p(n)$, which is true
        if $n$ is a prime number and false otherwise.
        Refer to the operator \emph{lf}
        from Exercise \ref{largest-factor} in your definition.\\
        \emph{Note}: A prime number is a natural number greater than or equal to two
        that has no factors other than one and itself.

\Exercise Define an operator to compute $rp(n)$,
the sum of the reciprocals of all the prime numbers that are less than or equal to
a given natural number $n$.
Use the equations that define the operator
$h$ (page~\pageref{reciprocalsdef})
as a model for your definition of $rp(n)$.\footnote{The
\index{harmonic series}harmonic \index{series, harmonic}series, $h(n)$,
grows without bound as $n$ becomes large,
but very slowly, at about the same rate of growth as $log(n)$.
The number $rp(n)$ grows even more slowly, of course,
but it, too, grows without bound.
However, the sum of the squares of the reciprocals is bounded
by $\pi^2/6$, which is about $1.64$.
Leonhard Euler proved these facts over 200 years ago, during
a kind of Cambrian explosion of mathematics initiated a hundred years earlier
by Isaac \index{Newton, Isaac}Newton and Gottfried \index{Leibniz, Gottfried}Leibniz
with their invention of the infinitesimal calculus.
In their development of calculus,
Newton and Leibniz conjured up mathematical entities known as infinitesimal numbers
to justify the new mathematics that they introduced in the 1600s,
but it wasn't until the 1960s that Abraham \index{Robinson, Abraham}Robinson invented a formal logic
for reasoning about infinitesimal numbers.
The proof engine \index{ACL2r}ACL2r, which is an extension of ACL2,
the mechanized logic used extensively in this book,
employs Robinson's \index{non-standard analysis}non-standard analysis
to support partially automated, formal reasoning about functions with
numeric operands of infinite precision.}\\
\emph{Hint}: Use three equations, one for numbers $n$ that are prime numbers,
another for non-zero numbers that are not prime,
and a third for the case when $n = 0$.
It will be helpful to refer to the operator $p$ from Exercise \ref{prime-predicate}.
\end{ExerciseList}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "book"
%%% End:
