\chapter{Mathematical Induction}

\section{Predicates and Lists as Mathematical Objects}
\label{sec:lists-as-obj}
A sequence is an ordered list of elements. 
In fact, for our purposes, the terms ``list'' and ``sequence'' are synonyms,
and we will more often use the term ``list'' for this kind of mathematical object.
Our notation for lists displays the elements of the sequence separated by spaces,
and the notation requires parentheses marking the beginning and end of the list. 
For example, the formula ``(8 3 7)'' denotes the list with first element 8, 
second element 3, and third element 7. 
The formula ``(9 8 3 7)'' denotes a list with the same elements, 
plus an additional element ``9'' at the beginning. 

\label{nil-def}
We use the symbol ``nil'' for the empty list (that is, the list with no elements).

We will start with a few basic operators for lists. 
One of them, the list construction operator, ``cons'', 
inserts a new element at the beginning of a list. 
Formulas using ``cons'', like all formulas in 
the mathematical notation we have been using to discuss software concepts, 
are written in prefix form. 
So, the formula ``(cons $x$ $xs$)'' denotes the list 
with the same elements as the list $xs$, 
but with an additional element $x$ inserted at the beginning. 
If $x$ stands for the number ``9'', 
and $xs$ stands for the list ``(8 3 7)'', 
then ``(cons $x$ $xs$)'' constructs the list ``(9 8 3 7)''.

Any list can be constructed by starting from the empty list 
and using the construction operator to insert the elements of the list, one by one. 
For example, the formula ``(cons 8 (cons 3 (cons 7 nil)))'' 
is another notation for the list ``(8 3 7)''. 
In fact, using the operator ``cons'' is the only way to construct non-empty lists. 
The empty list ``nil'' is given. 
All other lists (that is, all non-empty lists) are constructed using the ``cons'' operator. 
The formula ``(8 3 7)'' is shorthand for ``(cons 8 (cons 3 (cons 7 nil)))''.

The operator ``consp'' checks for non-empty lists.
That is, the formula ``(consp $xs$)'' delivers true 
if $xs$ is a non-empty list and false otherwise. 
The \{\emph{cons}\} axiom of list construction is a 
formal statement of the fact that all non-empty lists 
are constructed with the ``cons'' operator.
\label{cons-axiom-formal}
\begin{center}
Axiom \{\emph{cons}\} \\
$(\forall xs.($(consp $xs$) $=$ ($\exists y.$ ($\exists ys.$ ($xs$ = (cons $y$ $ys$)$)))))$
\end{center}

Wait a minute! Where do those funny-looking symbols come from.
What do upside-down ``A'' and backwards ``E'' mean?
Let's start with the backwards ``E''.
The operator
\label{exists-def}
``$\exists$'' converts a collection of true/false formulas 
into a single true/false formula.

\label{proposition-def}
We will use the term ``proposition'' to mean ``true/false formula''.
So, ``$\exists$'' converts a collection of propositions
into a single proposition.
The collection of propositions is indexed (or ``parameterized'')
by a variable that ranges across a 
\label{universe-def}
``universe of discourse.''
For each element in the universe of discourse,
there is a corresponding proposition in the collection.

Any collection of propositions 
indexed by the elements of a universe of discourse is called,
when the collection is taken as a whole, a ``predicate''.
Let $P(xs, y, ys)$ be shorthand for the equation $xs$ = (cons $y$ $ys$).
Given a particular list $xs$ together with a particular entity $y$, 
we can view the equation $P(xs, y, ys)$ as a collection of propositions
indexed by the variable $ys$, whose universe of discourse is a collection of lists.

In other words, if $xs$ and $ys$ denote lists and 
$y$ denotes an object of some kind,
then the equation $P(xs, y, ys)$ must be either true or false.
For example, if $xs$ denotes the list ``(1 2 3)'' 
and $y$ denotes the object ``1'',
then $P(xs, y, ys)$ stands for $P($(1 2 3), 1, $ys)$
which is an equation involving the variable $ys$.
There is one such equation for each possible list $ys$,
and taken all together those equations comprise a predicate. 

The formula 
$(\exists ys.P($(1 2 3), 1, $ys))$ denotes true
if there is some list $ys$ 
for which the equation (1 2 3) = (cons $1$ $ys$) is valid.
If there were no such list, the formula $P($(1 2 3), 1, $ys)$  
would denote false.
In this case, $(\exists ys.P($(1 2 3), 1, $ys))$ denotes true
because when $ys$ is the list ``(2 3)'', the
formula $P($(1 2 3), 1, $ys)$ stands for the equation 
(1 2 3) $=$ (cons 1 (2 3)), which is true.

If, on the other hand, $xs$ were the list ``(1 2 3)''
and $y$ were the number ``2'', there would be no list
$ys$ that would make the equation (1 2 3) = (cons $2$ $ys$) valid
because the list on the left-hand side of the equation
starts with 1 the the one on the right-hand side starts with 2.
So, the formula $(\exists ys.P($(1 2 3), 2, $ys))$
is false.

More generally, if we take $xs$ to stand a particular list
and $y$ to stand for a particular object, then
$P(xs, y, ys)$ is an equation that is either true or false.
That makes $P(xs, y, ys)$ a different proposition 
for each possible value of the variable $ys$,
and $(\exists ys.P(xs, y, ys))$, which is
shorthand for 
($\exists ys.$ ($xs$ = (cons $y$ $ys$))),
is true if there is a list $ys$ that makes the
equation $xs$ = (cons $y$ $ys$) valid and false if there is no such list $ys$.

We refer to the variable $ys$ in the formula $(\exists ys.P($(1 2 3), 2, $ys))$
as the ``bound variable'' corresponding to the $\exists$ operator in the formula.
Likewise, $ys$ is the bound variable corresponding 
to the $\exists$ operator in the formula
($\exists ys.$ ($xs$ = (cons $y$ $ys$))).

Every $\exists$ operator that occurs in a formula must be 
followed immediately by a variable, then a period.
The variable between $\exists$ operator and the period is known as the 
\label{bound-var-def} 
``bound variable''
associated with the $\exists$ operator.
\label{quantify-def} 
The $\exists$ operator ``quantifies'' the formula after the period
with respect to the universe of discourse of the variable before the period.

Now, let's take a step back.
We can view the formula
($\exists ys.$ ($xs$ = (cons $y$ $ys$)))
as a collection of propositions,
one for each object $y$ from the universe of discourse for $y$.
The formula
$(\exists ys.P(xs, y, ys))$ is a shorthand for that
collection of propositions.
Since any collection of propositions is a predicate,
we can view $(\exists ys.P(xs, y, ys))$ as a predicate indexed
by the objects that $y$ can stand for 
(that is, the objects in the universe of discourse for $y$).

We can convert the predicate $(\exists ys.P(xs, y, ys))$
into a true/false value (that is, convert it to a proposition)
by applying the $\exists$ operator again,
but this time with $y$ as the bound variable:
$(\exists y.(\exists ys.P(xs, y, ys)))$.
Again, this formula is a different proposition for each
possible list that $xs$ can stand for.
This collection of propositions is a predicate,
and we can apply a quantifier to the predicate to convert it to a proposition.

The only quantifier we've discussed is the $\exists$ operator.
We could use it again, but we don't want to.
We want to use a new quantifier, the one signified
the the upside-down ``A'': the $\forall$ operator.
\label{forall-def}
If Q is a predicate and $x$ is a variable standing for
an element of the universe of discourse of Q,
then the formula $(\forall x.Q(x))$ stands for true
if there are no elements $x$ in the universe of discourse
for which $Q(x)$ is false. 
Otherwise, that is if there is a value $x$ such that $Q(x)$ is false,
then $(\forall x.Q(x))$ is false.

When we apply $\forall$ quantification to the formula
$(\exists y.(\exists ys.P(xs, y, ys)))$,
we get the formula
$(\forall xs.(\exists y.(\exists ys.P(xs, y, ys))))$.
Of course, that formula is shorthand for
$(\forall xs.(\exists y.$ ($\exists ys.$ ($xs$ = (cons $y$ $ys$))))),
which is the formula defining the \{\emph{cons}\} axiom.
The meaning of the \{\emph{cons}\} axiom, therefore,
is that the formula (cons $xs$) always has the same value as the formula
($\exists y.$ ($\exists ys.$ ($xs$ = (cons $y$ $ys$)))),
regardless of what list $xs$ denotes.

We will often cite the \{\emph{cons}\} axiom to write 
a formula like (cons $x$ $xs$) in place of any list we know is not empty.
When we do this, we will take care to choose the symbols $x$ and $xs$ 
to avoid conflicts with other symbols that appear in the context of the discussion.
Furthermore, we will often cite a less formal version of the \{\emph{cons}\} axiom 
when we know we are dealing with a non-empty list. 
For example, the list ($x_1$ $x_2$ \dots $x_{n+1}$) 
cannot be empty because it has $n+1$ elements, and $n+1$ 
is at least one when $n$ is a natural number. 
(We will always assume that variables appearing in subscripts are natural numbers.)
\begin{center}
\label{cons-axiom-informal}
Axiom \{\emph{cons}\} (informal version) \\
($x_1$ $x_2$ \dots $x_{n+1}$) = (cons $x_1$ ($x_2$ \dots $x_{n+1}$))
\end{center}

The construction operator, ``cons'', cannot be the whole story, of course. 
To compute with lists, we  need to be able to construct them, 
but we also need to be able to take them apart. 
There are two basic operators for taking lists apart: ``first'' and ``rest''. 
We express the relationship between these operators and 
the construction operator in the form of equations 
(\{\emph{first}\} and \{\emph{rest}\}), 
along with the informal version of the \{\emph{cons}\} axiom.
\begin{center}
\label{first-rest-cons}
 Axioms \{\emph{cons}\}, \{\emph{first}\}, and \{\emph{rest}\} \\
\begin{tabular}{ll}
 ($x_1$ $x_2$ \dots $x_{n+1}$) = (cons $x_1$ ($x_2$ \dots $x_{n+1}$)) & \{\emph{cons}\} \\
 (first (cons $x$ $xs$)) = $x$                                        & \{\emph{first}\}\\
 (rest (cons $x$ $xs$))  = $xs$                                       & \{\emph{rest}\}
\end{tabular}
\end{center}
The \{\emph{first}\} axiom is a formal statement of the fact that 
the operator ``first'' delivers the first element from non-empty list. 
The \{\emph{rest}\} axiom states that the operator ``rest'' delivers 
a list like its argument, but without the first element. 
Note that the lists in the \{\emph{first}\} and \{\emph{rest}\} 
axioms have at least one element because they are constructed by the cons operator.

We will use equations like the ones in these axioms in the 
same way we used the logic equations in Figure~\ref{fig-02-02} 
(page \pageref{fig-02-02}) and the arithmetic equations of 
Figure~\ref{fig-02-01} (page \pageref{fig-02-01}).
That is, whenever we see a formula like ``(first (cons $x$ $xs$))'',
no matter what formulas $x$ and $xs$ stand for, 
we will be able to cite equation \{\emph{first}\} to replace 
``(first (cons $x$ $xs$))'' by the simpler formula ``$x$''. 
Vice versa, we can also cite equation \{\emph{first}\} 
to replace any formula ``$x$'' by the more complicated formula 
``(first (cons $x$ $xs$))''. 
Furthermore, the formula ``$xs$'' in the replacement can be 
any formula we care to make up, as long as it is grammatically correct.

Similarly, we can cite the equation \{\emph{rest}\} to justify 
replacing the formula ``(rest (cons $x$ $xs$))'' by ``$xs$'' 
and vice versa, regardless of what formulas the symbols ``$x$'' and ``$xs$'' stand for. 
In other words, these are ordinary algebraic equations. 
The only new factors are
(1)~the kind of mathematical object they denote (lists, instead of numbers or True/False propositions), and
(2)~the syntactic quirk of prefix notation (instead of the more familiar infix notation).

All properties of lists, as mathematical objects, derive from the \{cons\}, \{first\}, and \{rest\} axioms. 
For example, suppose there is an operator called ``len'' 
that delivers the number of elements in a list. 
We can use check-expect to test len in some specific cases.

\begin{lstlisting}
(check-expect (len (cons 8 (cons 3 (cons 7 nil)))) 3)
(check-expect nil 0)
\end{lstlisting}

We can use the doublecheck facility to automate tests. 
We expect that the number of elements in a non-empty list 
is one more than the number of elements remaining in the list 
after the first one is dropped using the ``rest'' operator. 
The following property tests this expectation.

\begin{lstlisting}
(defproperty len-test
  (xs :value (random-list-of (random-natural)))
  (= (len xs)
     (if (consp xs)
         (+ (len (rest xs)) 1)
         0)))
\end{lstlisting}

\begin{comment} ...suppressing defthm for now...
When a property holds under all circumstances, we can sometimes use the automated logic of ACL2 to prove it. To do so, we formulate the property as a theorem and press the ``Start'' button in the Dracula proof panel (right side of Dracula window). When the ``ACL2!\verb+>+'' prompt appears in the lower pane in the proof panel, we press the ``Admit'' arrow, and the automated logic of ACL2 starts trying to prove the theorem.

Theorem definitions are similar to property definitions, but the keyword is ``defthmd'' instead of ``defproperty''. The following theorem definition states the len-test property in a form that the automated logic of ACL2 can use to attempt a proof that the property holds under all circumstances.

\label{len-thm}
\begin{lstlisting}
(defthmd len-thm
  (= (len xs)
     (if (consp xs)
         (+ 1 (len (rest xs))) ; {len1}
         0)))                  ; {len0}
\end{lstlisting}

ACL2 interprets variables in theorems as if they were universally quantified. So, the formula ``(= (len $xs$) (if (consp $xs$) (+ 1 (len (rest $xs$))) 0))'' in the definition of len-thm means ``($\forall$$xs$.(= (len $xs$) (if (consp $xs$) (+ 1 (len (rest $xs$))) 0)))''.
In this case, ACL2 successfully proves the theorem, and Dracula colors the theorem green. (If ACL2 had failed to prove the theorem, Dracula would have colored it pink.) Because ACL2 succeeds in proving the theorem, we know that the ``len-test'' property from our doublecheck testing is true under all circumstances. We can cite this fact in proofs.

The len theorem contains two formulas that have the same meaning as (len $xs$). One of them, which we have labeled ``\{\emph{len1}\}'', applies when the argument in an invocation of len is a list with at least one element (that is, (consp $xs$) is true).  The other formula, which we have labeled ``\{\emph{len0}\}'', applies when the argument is the empty list (nil).
\end{comment}

This property holds under all circumstances. 
We can express the idea in the form of equations 
that serve as axioms for the len operator.
\begin{center}
Axioms \{\emph{len}\} \\
\begin{tabular}{ll}
(len nil) = 0                            & \{\emph{len0}\} \\
(len (cons $x$ $xs$)) = (+ (len $xs$) 1) & \{\emph{len1}\}
\label{len-equations}
\end{tabular}
\end{center}

\begin{comment}
We also expect the ``len'' operator to deliver a natural number, regardless of what its argument is. We can state this in the form of a theorem using the ``natp'' operator, which delivers true if its argument is a natural number and false if it isn't.

\label{len-nat-thm}
\begin{lstlisting}
(defthmd len-is-natural-number-thm
  (natp (len xs)))
\end{lstlisting}

ACL2 succeeds in proving this theorem, too, so we now know that the formula (len $xs$) delivers a non-negative integer, regardless of what formula $xs$ stands for. We will use the label \{\emph{len-nat}\} when we cite this theorem in proofs.

A related fact is that the formula (consp $xs$) is logically equivalent to the formula (\verb+>+ (len $xs$) 0). In the notation from Chapter~\ref{ch:Boolean-Formulas}: (consp $xs$)$\leftrightarrow$(\verb+>+ (len $xs$) 0). The name of the equivalence operator in ACL2 is ``iff'', so in ACL2 notation, the formula would be:
(iff (consp $xs$) (\verb+>+ (len $xs$) 0)). Or, stated as a theorem, it looks like this:

\begin{lstlisting}
(defthmd consp<->len>0-thm
  (iff (consp xs) (> (len xs) 0)))
\end{lstlisting}
\end{comment}

We expect the ``len'' operator to deliver a natural number, 
regardless of what its argument is.
For the record, we state this property as a theorem. 
Later, you will have a chance to derive this theorem from the \{\emph{len}\} axioms. 
The theorem refers to the natp operator, 
which you have seen before (page \pageref{natp-op}). 
It delivers true if its argument is a natural number and false otherwise.
\begin{center}
\label{len-nat-thm}
Theorem \{\emph{len-nat}\} \\
$\forall xs.$(natp (len $xs$))
\end{center}

We can derive this property of len from its axioms, 
but instead of plodding through that derivation at this point,
we are going to proceed to some more interesting issues. 
A related fact is that the formula (consp $xs$) always has the same value 
as the formula ($>$ (len $xs$) 0). %(\verb+>+ (len $xs$) 0). %\textit{using math mode instead of \verb}
%In the notation from Chapter~\ref{ch:Boolean-Formulas}:  %\textit{never covered the equiv op}
%(consp $xs$)$\leftrightarrow$(\verb+>+ (len $xs$) 0). 
This theorem, too, can be derived from the \{\emph{len}\} axioms, 
but we will take a pass on proving the theorem, for the moment, 
and state it without proof.
\begin{center}
\label{consp-len-thm}
Theorem \{\emph{consp}$=$len$>$0\} \\
$\forall xs.($(consp $xs$) $=$ ($>$ (len $xs$) 0)$)$
\end{center}

\section{Mathematical Induction}
\label{sec:induction}
The cons, first, and rest operators form the basis for computing with lists, 
but there are lots of other operators, too. 
For example, consider an operator ``append'' that concatenates two lists. 
We describe this operator using an informal schematic for lists 
that labels the elements of the list as subscripted variables. 
The number of subscripts in the sequence implicitly reveals the number of elements in the list.

\label{list-schematic} In the following list schematics, the ``$x$'' list has $m$ elements, the ``$y$'' list has $n$ elements, and the concatenated list has $m+n$ elements.
\begin{center}
(append ($x_1$ $x_2$ \dots $x_m$) ($y_1$ $y_2$ \dots $y_n$)) = ($x_1$ $x_2$ \dots $x_m$ $y_1$ $y_2$ \dots $y_n$)
\end{center}

Some simple tests might bolster our understanding of the operator.

\begin{lstlisting}
(check-expect (append '(1 2 3 4) '(5 6 7)) '(1 2 3 4 5 6 7))
(check-expect (append '(1 2 3 4 5) nil) '(1 2 3 4 5))
\end{lstlisting}

\begin{aside}
What is the single-quote mark doing in the formula '(1 2 3 4)? 
It is there to avoid confusing lists with computational formulas. 
By default, the ACL2 system interprets a formula like 
(f $x$ $y$ $z$) as an invocation of the operator ``f'' with operands $x$, $y$, and $z$. 
ACL2 interprets the first symbol it encounters after a left parenthesis 
as the name of an operator, and it interprets the other formulas, 
up to the matching right parenthesis, as operands.
So, ACL2 interprets the ``1'' in the formula (1 2 3 4) as the name of an operator.
Because there is no operator with the name ``1'', the interpretation fails.

If we want to specify the list ``(1 2 3 4)'' in a formula, 
we can, of course, use the cons operator to construct it: 
(cons 1 (cons 2 (cons 3 (cons 4 nil)))). 
But, that's too bulky for regular use. 
The single-quote trick provides a shorthand: 
'(1 2 3 4) has the same meaning as the bulky version.
The single-quote mark suppresses the default interpretation 
of the first symbol after the left-parenthesis and 
delivers the list whose elements are in the parentheses. 
Without the single-quote mark,  
%the ``1'' in ``(1 2 3 4)'' would be interpreted as an operator, and because there is no operator named ``1'', 
the formula would make no sense.
\caption{Single-quote Shorthand for Lists}
\label{quote}
\end{aside}

We can use doublecheck for more extensive testing. 
If we concatenate the empty list nil with a list $ys$, 
we expect to get $ys$ as a result: (append nil $ys$) = nil. 
If we concatenate a non-empty list $xs$ with a list $ys$, 
we expect the first element of the result to be the same as 
the first element of $xs$. 
Furthermore, we expect the rest of the elements to be 
the elements of the list we would get if we concatenated 
a list made up of the other elements of $xs$, that is (rest $xs$),  
with $ys$. The following property definition expresses this idea formally.

\begin{samepage}
\begin{lstlisting}
(defproperty append-test
  (xs :value (random-list-of (random-natural))
   ys :value (random-list-of (random-natural)))
  (equal (append xs ys)
         (if (consp xs)
             (cons (first xs)
                   (append (rest xs) ys))
             ys)))
\end{lstlisting}
\end{samepage}

\begin{aside}
Why does the property say ``(equal (append $xs$ $ys$) \dots)'' instead of ``(= (append $xs$ $ys$) \dots)''? the ``='' operator is restricted to numbers. The ``equal'' operator can check for equality between other kinds of objects. You can always use ``equal'', but you can only use ``='' when both operands are numbers. Why bother with ``='', when its use is so limited? We might say it makes the formula look more like an equation, but that's not really much of an excuse, since we have already had to conform to prefix notation instead of the more familiar infix notation. So, feel free to use the ``equal'' operator all the time if you want to. 
%We will be using ``='' when we can and hope it doesn't put too much of an extra burden on you.
\caption{``equal'' vs ``=''}
\label{equal}
\end{aside}

\begin{comment}
This might not be the first test you would think of, but if the test failed to pass, you would for sure know something was wrong with the append operator.
This is another property that ACL2 can prove when it is stated as a theorem.

\begin{lstlisting}
(defthmd append-thm
  (equal (append xs ys)
         (if (consp xs)
             (cons (first xs)            ; {app1}
                   (append (rest xs) ys))
             ys)))                       ; {app0}
\end{lstlisting}
\end{comment}

The append-test property might not be the first test you would think of, 
but if the test failed to pass, 
you would for sure know something was wrong with the append operator. 
In fact the property is so plainly correct, 
we are going to state it in the form of equations that we accept as axioms.

Like the \{\emph{len}\} theorem, there are two \{\emph{append}\} equations, 
and they specify the meaning of the append operation in different situations. 
One of them specifies the meaning when the first argument is the empty list,
the other when the list has one or more elements 
(that is, when the list is constructed by the cons operator).

\begin{center}
\label{append-equations}
Axioms: \{\emph{append}\} \\
\begin{tabular}{ll}
(append nil $ys$) =  $ys$                                     & \{\emph{app0}\} \\
(append (cons $x$ $xs$) $ys$) = (cons $x$ (append $xs$ $ys$)) & \{\emph{app1}\} \\
\end{tabular}
\end{center}

These equations about the append operation are simple enough, 
but it turns out that lots of other properties of the 
append operation can be derived from them. 
For example, we can prove that the length of 
the concatenation of two lists is the sum of the lengths of the lists. 
We call this theorem the ``additive law of concatenation''. 
Let's see how a proof of this law could be carried out.

First, let's break it down into a some special cases. 
We will use L($n$) as shorthand for the proposition that 
(len (append ($x_1$ $x_2$ \dots $x_n$) $ys$)) 
is the sum of (len ($x_1$ $x_2$ \dots $x_n$)) and (len $ys$).
That makes L a predicate whose universe of discourse is
the natural numbers.

\begin{quote}
\begin{tabbing}
L($n$) $\equiv$ (= \=(len (append ($x_1$ $x_2$ \dots $x_n$) $ys$))  \\
                   \>(+ (len ($x_1$ $x_2$ \dots $x_n$)) (len $ys$)))
\end{tabbing}
\label{additive-concat-law-predicate}
\end{quote}

For the first few values of $n$, L($n$) would stand for the following equations.
% L(0) $\equiv$ (= (len (append nil $ys$)) (+ (len nil) (len $ys$))) \\
% L(1) $\equiv$ (= (len (append ($x_1$) $ys$)) (+ (len ($x_1$)) (len $ys$))) \\
% L(2) $\equiv$ (= (len (append ($x_1$ $x_2$) $ys$) (+ (len ($x_1$ $x_2$)) (len $ys$))) \\
% L(3) $\equiv$ (= (len (append ($x_1$ $x_2$ $x_3$) $ys$)) (+ (len ($x_1$ $x_2$ $x_3$)) (len $ys$))) \\
% L(4) $\equiv$ (= (len (append ($x_1$ $x_2$ $x_3$ $x_4$) $ys$)) (+ (len ($x_1$ $x_2$ $x_3$ $x_4$)) (len $ys$)))

\begin{center}
\begin{tabular}{llll}
L(0) & $\equiv$ & (= &(len (append nil $ys$)) \\
     &          &    &(+ (len nil) (len $ys$))) \\
L(1) & $\equiv$ & (= &(len (append ($x_1$) $ys$)) \\
     &          &    &(+ (len ($x_1$)) (len $ys$))) \\
L(2) & $\equiv$ & (= &(len (append ($x_1$ $x_2$) $ys$) 	\\
	 &          &    &(+ (len ($x_1$ $x_2$)) (len $ys$))) \\
L(3) & $\equiv$ & (= &(len (append ($x_1$ $x_2$ $x_3$) $ys$)) \\
     &          &    &(+ (len ($x_1$ $x_2$ $x_3$)) (len $ys$))) \\
L(4) & $\equiv$ & (= &(len (append ($x_1$ $x_2$ $x_3$ $x_4$) $ys$)) \\
     &          &    &(+ (len ($x_1$ $x_2$ $x_3$ $x_4$)) (len $ys$)))
\end{tabular}
\end{center}

We can derive L(0) from the \{\emph{append}\} and \{\emph{len}\} axioms as follows, 
starting from the first operand in the equation that L(0) stands for 
(the left-hand side, if the equation were written in the conventional way rather than prefix form), 
and ending with the second operand (right-hand side).

\begin{center}
\begin{tabular}{lll}
    & (len (append nil $ys$))  &                                                \\
$=$ & (len $ys$)               & \{\emph{app0}\}     (page \pageref{append-equations})\\
$=$ & (+ (len $ys$) 0)         & \{$+$ identity\}    (page \pageref{fig-02-01}) \\
$=$ & (+ 0 (len $ys$))         & \{$+$ commutative\} (page \pageref{fig-02-01}) \\
$=$ & (+ (len nil) (len $ys$)) & \{\emph{len0}\}     (page \pageref{len-equations})
\end{tabular}
\end{center}

That was easy. How about L(1)?

\begin{center}
\begin{tabular}{lll}
    & (len (append ($x_1$) $ys$))           &                     \\
$=$ & (len (append (cons $x_1$ nil) $ys$)   & \{\emph{cons}\} (\pageref{first-rest-cons}) \\
$=$ & (len (cons $x_1$ (append nil $ys$)))  & \{\emph{app1}\}     \\
$=$ & (+ 1 (len (append nil $ys$)))         & \{\emph{len1}\}     \\
$=$ & (+ 1 (+ (len nil) (len $ys$)))        & \{L(0)\}            \\
$=$ & (+ (+ 1 (len nil)) (len $ys$))        & \{$+$ associative\} (page \pageref{fig-02-01}) \\
$=$ & (+ (len (cons $x_1$ nil)) (len $ys$)) & \{\emph{len1}\}     \\
$=$ & (+ (len ($x_1$) (len $ys$))           & \{\emph{cons}\}     \\
\end{tabular}
\end{center}

That was a little harder. Will proving L(2) be still harder? Let's try it.

\begin{center}
\begin{tabular}{lll}
    & (len (append ($x_1$ $x_2$) $ys$))         &                     \\
$=$ & (len (append (cons $x_1$ ($x_2$)) $ys$))  & \{\emph{cons}\}     \\
$=$ & (len (cons $x_1$ (append ($x_2$) $ys$)))  & \{\emph{app1}\}     \\
$=$ & (+ 1 (len (append ($x_2$) $ys$)))         & \{\emph{len1}\}     \\
$=$ & (+ 1 (+ (len ($x_2$)) (len $ys$)))        & \{L(1)\}            \\
$=$ & (+ (+ 1 (len ($x_2$))) (len $ys$))        & \{$+$ associative\} \\
$=$ & (+ (len (cons $x_1$ ($x_2$))) (len $ys$)) & \{\emph{len1}\}     \\
$=$ & (+ (len ($x_1$ $x_2$)) (len $ys$))        & \{\emph{cons}\}     \\
\end{tabular}
\end{center}

Fortunately, proving L(2) was no harder than proving L(1). 
In fact the two proofs cite exactly the same equations all the way through, 
except in one place. 
Where the proof of L(1) cited the equation L(0), 
the proof of L(2) cited the equation L(1). 
Maybe the proof of L(3) will work the same way.

\begin{center}
\begin{tabular}{lll}
    & (len (append ($x_1$ $x_2$ $x_3$) $ys$))         &                     \\
$=$ & (len (append (cons $x_1$ ($x_2$ $x_3$)) $ys$))  & \{\emph{cons}\}     \\
$=$ & (len (cons $x_1$ (append ($x_2$ $x_3$) $ys$)))  & \{\emph{app1}\}     \\
$=$ & (+ 1 (len (append ($x_2$ $x_3$) $ys$)))         & \{\emph{len1}\}     \\
$=$ & (+ 1 (+ (len ($x_2$ $x_3$)) (len $ys$)))        & \{L(2)\}            \\
$=$ & (+ (+ 1 (len ($x_2$ $x_3$))) (len $ys$))        & \{$+$ associative\} \\
$=$ & (+ (len (cons $x_1$ ($x_2$ $x_3$))) (len $ys$)) & \{\emph{len1}\}     \\
$=$ & (+ (len ($x_1$ $x_2$ $x_3$)) (len $ys$))        & \{\emph{cons}\}     \\
\end{tabular}
\end{center}

By now, it's easy to see how to derive L(4) from L(3), 
then L(5) from L(4), and so on. 
If you had the time and patience, you could surely prove L(100), L(1000), or even L(1000000) 
by deriving the next one from the one you just finished proving, 
following the established pattern. 
We could even write a program to print out the proof of L($n$), given any natural number $n$.

Since we know how to prove L($n$) for any natural number $n$, 
it seems fair to say that we know all those equations are true. 
That is, we think we know that the formula ($\forall$$n$.L($n$)) is true.
However, to complete proof of that formula, 
we need a rule of inference that allows us to make conclusions 
from patterns like those we observed in proving L(1), L(2), and so on. 
That rule of inference is known as ``mathematical induction''.

Mathematical induction provides a way to prove that 
formulas like ($\forall$$n$.P($n$)) are true 
when P is a predicate whose universe of discourse is the natural numbers. 
If for each natural number $n$, P($n$) stands for a proposition, 
then mathematical induction is an applicable inference rule 
in a proof that is ($\forall$$n$.P($n$)) true. 
That is not to say that such a proof can be constructed. 
It's just that mathematical induction might provide some help in the process.
The inverse is also true: mathematical induction cannot help
if the universe of discourse is not the natural numbers.

The rule goes as follows: one can infer the truth of ($\forall$$n$.P($n$)) 
from proofs of two other propositions. 
Those two propositions are P(0) and ($\forall$$n$.(P($n$)$\rightarrow$P($n+1$))). 
It's a very good deal if you think about it. 
A direct proof of ($\forall$$n$.P($n$)) would require a proof of proposition P($n$) 
for each value of $n$ (0, 1, 2, \dots). 
But, in a proof by induction, the only proposition that needs to be proved on its own is P(0). 
In the proof any of the other propositions, 
you are allowed to cite the previous one in the sequence as a justification for any step in the proof.

The reason you can assume that P($n$) is true in the proof of P($n+1$) 
is because the goal is to prove that the formula 
P($n$)$\rightarrow$P($n+1$) has the value ``true''. 
We know from the truth table of the implication operator 
(page \pageref{implication-truth-table}) that the implication
P($n$)$\rightarrow$P($n+1$) is true when P($n$) is false,
regardless of the value of P($n+1$).
So, we only need to verify that the formula is true when P($n$) is true. 
The implication will be true in this case only if P($n+1$) is true. 
So, we need to prove that P($n+1$) under the assumption that P($n$) is true.
\begin{figure}
\begin{center}
\begin{tabular}{l}
Prove P(0) \\
Prove ($\forall$$n$.(P($n$)$\rightarrow$P($n+1$))) \\
\hline
Infer ($\forall$$n$.P($n$))
\end{tabular}
\end{center}
\caption{Mathematical Induction--a rule of inference}
\label{fig-04-01}
\end{figure}

That is, in the proof of P($n+1$), you can cite P($n$) to justify any step in the proof. 
\label{induction-hyp-def}
P($n$) gives you a leg up in the proof of P($n+1$) and is known as the ``induction hypothesis''.
Now, let's apply mathematical induction to prove 
the additive law of concatenation. 
Here, the predicate that we will apply the method to is L (page \pageref{additive-concat-law-predicate}).

\label{len-additive-thm}
We have already proved L(0). 
All that is left is to prove ($\forall$$n$.(L($n$)$\rightarrow$L($n+1$))). 
That is, we have to derive L($n+1$) from L($n$) for an arbitrary natural number $n$. 
Fortunately, we know how to do this. Just copy the derivation of, 
say L(3) from L(2), but start with an append formula in which the first operand 
is a list with $n+1$ elements, and cite L($n$) where we would have cited L(3).

\begin{center}
\begin{tabular}{lll}
    & (len (append ($x_1$ $x_2$ \dots $x_{n+1}$) $ys$))         &                     \\
$=$ & (len (append (cons $x_1$ ($x_2$ \dots $x_{n+1}$)) $ys$))  & \{\emph{cons}\}     \\
$=$ & (len (cons $x_1$ (append ($x_2$ \dots $x_{n+1}$) $ys$)))  & \{\emph{app1}\}     \\
$=$ & (+ 1 (len (append ($x_2$ \dots $x_{n+1}$) $ys$)))         & \{\emph{len1}\}     \\
$=$ & (+ 1 (+ (len ($x_2$ \dots $x_{n+1}$)) (len $ys$)))        & \{L($n$)\}          \\
$=$ & (+ (+ 1 (len ($x_2$ \dots $x_{n+1}$))) (len $ys$))        & \{$+$ associative\} \\
$=$ & (+ (len (cons $x_1$ ($x_2$ \dots $x_{n+1}$))) (len $ys$)) & \{\emph{len1}\}     \\
$=$ & (+ (len ($x_1$ $x_2$ \dots $x_{n+1}$)) (len $ys$))        & \{\emph{cons}\}     \\
\end{tabular}
\end{center}

This completes the proof by mathematical induction of the
additive law of concatenation.
\begin{center}
\label{additive-law-concatenation}
Theorem \{\emph{additive law of concatenation}\} \\
$\forall$$n$.((len (append ($x_1$ $x_2$ \dots $x_n$) $ys$))
= (+ (len ($x_1$ $x_2$ \dots $x_n$)) (len $ys$)))
\end{center}

An important point to notice in this proof is that 
we could not cite the \{\emph{cons}\} equation to replace ($x_2$ \dots $x_{n+1}$) 
with (cons $x_2$ ($x_3$ \dots $x_{n+1}$)). 
The reason we could not do this is that we are trying to derive 
L($n+1$) from L($n$) without making any assumptions about $n$ 
other than the fact that it is a natural number. 
Since zero is a natural number, the list ($x_2$ \dots $x_{n+1}$) 
could be empty, and the cons operation cannot deliver an empty list as its value.

In the next section, we will prove some properties of append 
that confirm its correctness with respect to a specification in terms of other operators. 
These properties, and in fact all properties of the append operator, 
can be derived from the append axioms (page \pageref{append-equations}). 
Those axioms state properties of the append operation in two separate cases: 
(1)~when the first operand is the empty list (the \{\emph{app0}\} equation), and 
(2)~when the first operand is a non-empty list (the \{\emph{app1}\} equation). 
When the first operand is the empty list, 
the result must be the second operand, no matter what it is. 
When the first operand is not empty, it must have a first element. 
That element must also be the first element of the result. 
The other elements of the result are the ones you would get 
if you appended the rest of the first operand with the second operand.

Both of these properties are so straightforward and easy to believe 
that we would probably be willing to accept them as axioms, with no proof at all. 
It might come as a surprise that all of the other properties 
of the append operation can be derived from 
the two simple properties \{\emph{app0}\} and \{\emph{app1}\}. 
That is the power of mathematical induction. 
The two equations of the append axioms 
amount to an inductive definition of the append operator.

An inductive definition is circular in the sense 
that some of the equations in the definition refer 
to the operator on both sides of the equation. 
Most of the time, we think circular definitions are not useful, 
so it may seem surprising that they can be useful in mathematics. 
Some aren't, but some of them are, and you will 
gradually learn how to recognize and create useful, 
circular (that is, inductive) definitions.

\begin{figure}
\begin{center}
\begin{tabular}{lp{3.5in}}
\emph{Complete} & All possible combinations of operands are covered by at least one equation in the definition. \\
\emph{Consistent} & Combinations of operands covered by by two or more equations imply the same value for the operation. \\
\emph{Computational} & (1)~There is at least one non-inductive equation (that is, an equation in which the operator being defined
occurs only on the left-hand side). 
(2)~All invocations of the operator on the right-hand side of inductive equations 
have operands that are closer to the operands on the left-hand side of a non-inductive equation 
than the operands on the left-hand side of the inductive equation.
\end{tabular}
\caption{Key to Inductive Definition: The Three C's}
\end{center}
\label{fig:inductive-def-keys}
\end{figure}

It turns out that all functions that can be defined in software 
have inductive definitions in the style of the equations of the append axioms (page \pageref{append-equations}).
The keys to an inductive definition of an operator are listed in Figure~\ref{fig:inductive-def-keys}
(page \pageref{fig:inductive-def-keys}). All of the software we will discuss will take the form of a collection of inductive definitions of operators. That makes it possible to use mathematical induction as the fundamental tool in verifying properties of that software to a logical certainty.

This is not the only way to write software. 
In fact, most software is not written in terms of inductive definitions. 
But, properties of the software written using conventional methods cannot be derived using classicl logic. 
So, in terms of understanding what computers do and how they do it, 
inductive definitions provide solid footing. 
That is why we base our presentation on software written 
in terms of inductive definitions rather than conventional methods.


\section{Contatenation, Prefixes, and Suffixes}
\label{sec:append-prefix-suffix}
%%% in this section, prove the correctness of append
%%% with respect to a (prefix n xs) operator and (nthcdr n xs).

If you concatenate two lists, $xs$ and $ys$, you would expect to be able to retrieve the elements of $ys$ by dropping some of the elements of the concatenated lists. How many elements would you need to drop? That depends on the number of elements in $xs$. If there are $n$ elements in $xs$, and you drop $n$ elements from (append $xs$ $ys$), you expect the result to be identical to the list $ys$. We can state that expectation by using an intrinsic operation in ACL2 with the arcane name ``nthcdr''. The nthcdr operation takes two arguments: a natural number and a list. The formula (nthcdr $n$ $xs$) delivers a list like $xs$, but without its first $n$ elements. If $xs$ has fewer than $n$ elements, then the formula delivers the empty list.

\begin{comment}
ACL2 can prove this theorem, but the proof requires knowing something about the algebra of numbers. Fortunately, someone has worked out a basic theory of numeric algebra in ACL2 terms, and we can take advantage of that by importing it into our working environment. To do this, we use a command called ``include-book''. The name of the ``book'' with the theory we need is ``arithmetic/top'', and it resides in the ``system'' directory of ACL2.

\begin{lstlisting}
(include-book "arithmetic/top" :dir :system)
(defthmd append-suffix-thm
  (equal (nthcdr (len xs) (append xs ys))
         ys))
\end{lstlisting}

Let's see how we could prove this theorem. We would need to know some properties of the nthcdr operation. The following are three properties that are so easily believable, we are going to take them as axioms. The symbol $n$ stands for a natural number in these axioms. They would not be true if $n$ were, say, $-1$.
\end{comment}

The following equations state some simple properties of the nthcdr operation that we take as axioms.

\label{nthcdr-equations}
\begin{center}
Axioms \{\emph{nthcdr}\} \\
\begin{tabular}{ll}
(nthcdr 0 $xs$) = $xs$                                 & \{\emph{sfx0}\}     \\
(nthcdr $n$ nil) =  nil                                & \{\emph{sfx-nil}\}  \\
(nthcdr (+ $n$ 1) (cons $x$ $xs$)) = (nthcdr $n$ $xs$) & \{\emph{sfx1}\}     \\
\end{tabular}
\end{center}


\todo{Rex: sfx1 isn't true, right?  
I'm not sure we want to introduce it as an axiom, if later we'll have to explain it isn't really true.
Ruben: left as is for now, with (natp n) implicit, but inserted a comment about the type of n}

Given this background, we state the expected relationship 
between the append and nthcdr operators in terms of a sequence of special cases.
 We will use S($n$) as a shorthand for case number $n$. 
 There will be one case for each natural number.

\begin{center}
\begin{tabular}{lll}
S($n$) $\equiv$ (equal & (nthcdr & (len ($x_1$ $x_2$ \dots $x_n$))          \\
                       &         & (append ($x_1$ $x_2$ \dots $x_n$) $ys$)) \\
                       & $ys$)   &                                          \\
\end{tabular}
\end{center}

If S($n$) is true regardless of what natural number $n$ stands for, 
then the formula ($\forall$$n$.S($n$)) is true. 
Since the universe of discourse of the predicate S is the natural number,
mathematical induction may be useful in verifying that formula. 
All we need to do is to prove that 
(1)~the formula S(0) is true and 
(2)~the formula S($n+1$) is true under the assumption that S($n$) is true, 
regardless of what natural number $n$ stands for. Let's do that.

First, we prove S(0). 
When $n$ is zero, the list ($x_1$ $x_2$ \dots $x_n$) is empty, 
which is normally denoted by the symbol ``nil''. 
So, S(0) stands for the following equation.

\begin{center}
\begin{tabular}{ll}
S(0) $\equiv$ (equal & (nthcdr (len nil) (append nil $ys$)) \\
                     & $ys$)                                \\
\end{tabular}
\end{center}

Following our usual practice when proving an equation, we start with the formula on one side and use previously known equations to gradually transform that formula to the one on the other side of the equation.

\begin{center}
\begin{tabular}{lll}
    & (nthcdr (len nil) (append nil $ys$))  &                                                      \\
$=$ & (nthcdr (len nil) $ys$)               & \{\emph{app0}\} (page \pageref{append-equations})\\
$=$ & (nthcdr 0 $ys$)                       & \{\emph{len0}\} (page \pageref{len-equations})   \\
$=$ & $ys$                                  & \{\emph{sfx0}\}                                      \\
\end{tabular}
\end{center}

That takes care of S(0). Next, we prove S($n+1$), assuming that S($n$) is true.

\begin{center}
\begin{tabular}{lll}
S($n+1$) $\equiv$ (equal & (nthcdr & (len ($x_1$ $x_2$ \dots $x_{n+1}$))          \\
                         &         & (append ($x_1$ $x_2$ \dots $x_{n+1}$) $ys$)) \\
                         & $ys$)   &                                              \\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{llll}
    & (nthcdr & (len ($x_1$ $x_2$ \dots $x_{n+1}$))          & \\
    &         & (append ($x_1$ $x_2$ \dots $x_{n+1}$) $ys$)) & \\
$=$ & (nthcdr & (len (cons $x_1$ ($x_2$ \dots $x_{n+1}$)))          & \{\emph{cons}\} (page \pageref{cons-axiom-informal}) \\
    &         & (append (cons $x_1$ ($x_2$ $x_2$ \dots $x_{n+1}$)) $ys$)) & \{\emph{cons}\}                                    \\
$=$ & (nthcdr & (+ (len ($x_2$ \dots $x_{n+1}$)) 1)                 & \{\emph{len1}\} (page \pageref{len-equations})       \\
    &         & (cons $x_1$ (append ($x_2$ \dots $x_{n+1}$)) $ys$)) & \{\emph{app1}\} (page \pageref{append-equations})    \\
$=$ & (nthcdr & (len ($x_2$ \dots $x_{n+1}$))                       & \{\emph{sfx1}\}                                          \\
    &         & (append ($x_2$ \dots $x_{n+1}$) $ys$))              &                                                          \\
$=$ & $ys$    &                                                     & \{S($n$)\}                                              \\
\end{tabular}
\end{center}

The last step in the proof is justified by citing S($n$). 
This is a little tricky because the formula that S($n$) 
stands for is not exactly the same as the formula in the next-to-last step of the proof. 
We interpret the formula ($x_1$ $x_2$ \dots $x_n$) in the definition of S($n$) 
to stand for any list with $n$ elements. 
The elements in the list ($x_2$ \dots $x_{n+1}$) are numbered 2 through $n+1$, 
which means there must be exactly $n$ of them.

With this interpretation, the formula in the next-to-last step 
matches the formula in the definition of S($n$), 
which makes it legitimate to cite S($n$) to justify 
the transformation to $ys$ in the last step of the proof. 
We will use this interpretation frequently in proofs. 
We refer to it as the ``numbered-list interpretation'', or \{\emph{nlst}\} for short.

\label{numbered-list-interpretation}
\begin{center}
Numbered List Intepretation \{\emph{nlst}\} \\
($x_m$ \dots $x_n$) denotes a list with max($n-m+1$, 0) elements
\end{center}

\todo{Rex: We may want to skip this paragraph for now.  We can introduce this notation later, when needed. Ruben: right, done}
\begin{comment}
Sometimes, we will take this interpretation even further: ($x_m$ $x_{m+k}$ \dots $x_n$) stands for a list with
max($\lceil \frac{n - m + 1}{k} \rceil$, 0)
elements, where $\lceil x \rceil$ means the next integer that is $x$ or more. For example, both of the following formulas stand for the number 2: $\lceil \frac {4}{3}\rceil$, $\lceil \frac {4}{2}\rceil$. We will take the numbered-list interpretation \{\emph{nlst}\} to include this more general pattern, where subscripts in a numbered list may stride along leaving gaps of a certain, fixed size in between.

At this point, we know that (append $xs$ $ys$) delivers a list that has the right elements at the end. How about the beginning? We expect the concatenation to start with the elements of the list $xs$, so if we extract the first $n$ elements of (append $xs$ $ys$), where $n$ is (len $xs$), we would expect to get a list identical to $xs$. To express this expectation formally, we need a function that, given a number $n$ and a list $xs$, delivers the first $n$ elements of $xs$. Let's call that function ``prefix'' and think about properties it would have to satisfy.

Of course, if $n$ is zero, or if $xs$ is empty, (prefix $n$ $xs$) must be the empty list. If $n$ is non-zero natural number and $xs$ is not empty, then the first element of (prefix $n$ $xs$) must be the first element of $xs$, the the other elements must be the first $n-1$ elements of (rest $xs$). The following theorem puts these expectations in formal terms. The formula (posp $n$) refers to the intrinsic ACL2 operator ``posp''. It is true if $n$ is a non-zero natural number (that is, a strictly positive integer) and false otherwise.

\begin{lstlisting}
(defthmd prefix-thm
  (equal (prefix n xs)
         (if (and (posp n)
                  (consp xs))
             (cons (first xs)                 ; {pfx1}
                   (prefix (- n 1) (rest xs)))
             nil)))                           ; {pfx0}
\end{lstlisting}

These two equations are consistent, and they cover all the possibilities. They are also foundational and computational, in the sense of Figure \ref{fig:inductive-def-keys}. So, all properties that the prefix function satisfies can be derived from these equations. They comprise a definition of the function.
\end{comment}

At this point, we know that (append $xs$ $ys$) delivers 
a list that has the right elements at the end. 
How about the beginning?
We expect the concatenation to start with the elements of the list $xs$, 
so if we extract the first $n$ elements of (append $xs$ $ys$), where $n$ is (len $xs$), 
we would expect to get a list identical to $xs$. 
To express this expectation formally, we need a function that, 
given a number $n$ and a list $xs$, delivers the first $n$ elements of $xs$. 
Let's call that function ``prefix'' and think about properties it would have to satisfy.

Of course, if $n$ is zero, or if $xs$ is empty, 
(prefix $n$ $xs$) must be the empty list. 
If $n$ is non-zero natural number and $xs$ is not empty, 
then the first element of (prefix $n$ $xs$) must be the first element of $xs$, 
the the other elements must be the first $n-1$ elements of (rest $xs$). 
The following equations, which we take as axioms, 
put these expectations in formal terms. 
The formula (posp $n$) refers to the intrinsic ACL2 operator ``posp''. 
It is true if $n$ is a non-zero natural number 
(that is, a strictly positive integer) and false otherwise.

\label{prefix-equations}
\begin{center}
Axioms \{\emph{prefix}\}                                           \\
\begin{tabular}{ll}
(prefix 0 xs) = nil                          & \{\emph{pfx0}\}     \\
(prefix n nil) =  nil                        & \{\emph{pfx-nil}\}  \\
(prefix (+ n 1) (cons x xs)) = (prefix n xs) & \{\emph{pfx1}\}     \\
\end{tabular}
\end{center}

\begin{comment}
The prefix operator is not intrinsic in ACL2, but we can use ``defun'' to add it to the collection. In the definition, we provide a name for the function and state the equations that determine its computational properties. The equations take the same form in the definition as in a theorem, but a function definition must also specify the names it will use to refer the arguments of the function. Those names, which we refer to as the ``formal arguments'' or ``formal parameters'', appear as a list following the name of the function in the defun.

\begin{lstlisting}
(defun prefix (n xs)
  (if (and (posp n)
           (consp xs))
      (cons (first xs)                 ; {pfx1}
            (prefix (- n 1) (rest xs)))
      nil))                            ; {pfx0}
\end{lstlisting}

With this definition in place, you can observe the results of computing prefixes by entering formulas in the command panel of Dracula. That is the lower left panel, just below the one containing the definitions. The command panel is activated by pressing the Dracula Run button. Then, Dracula will respond to the formula (prefix 3 '(5 3 9 3 8)) with value (5 3 9). Try it out. Enter a formula invoking the prefix function and observe the response, just to get familiar with the operation.

We can state the prefix property of the append function in the form of a theorem. However, there is a technicality that needs to be discussed before stating the theorem. The cons operator produces a list when its second operand is a list, but it produces a different kind of object when its second argument is not a list. ACL2 provides an operator called ``true-listp'' that distinguishes between lists and other kinds of objects that the cons function can construct. So, (true-listp $xs$) is true when $xs$ is a list and false when it is some other kind of object.

When the first operand in the invocation of append is not a list, the prefix property fails to hold, and the theorem must take this into account. To do so, the theorem takes the form of an implication in which the hypothesis requires the first operand of append to be a list. Under that condition, it is correct to conclude that the prefix of the concatenation matches the first argument. That leads to the following formulation of the prefix property of the append operation.

\begin{lstlisting}
(defthmd append-prefix-thm
  (implies (true-listp xs)
           (equal (prefix (len xs) (append xs ys))
                  xs)))
\end{lstlisting}

ACL2 succeeds in proving this theorem on its own, but a little more practice with proof by mathematical induction won't hurt us, so let's do a paper-and-pencil proof ourselves. In our proof we will assume, as usual, that the notation ($x_1$ $x_2$ \dots $x_n$) stands for a list, so we don't need to worry about the hypothesis in the implication. It is automatically satisfied, and we can focus on the conclusion. As before, we will use P($n$) as a shorthand for special case number $n$.
\end{comment}

We can derive the prefix property of the append function from the equations for the prefix and append operations.
The proof will cite mathematical induction.  As before, we will use a shorthand for special case number $n$.

\todo{Rex: Should we replace xs with (x1 x2 ... xn). Ruben: right, done}

\begin{quote}
\begin{tabbing}
P($n$) $\equiv$ (equal \=(prefix \=(len ($x_1$ $x_2$ \dots $x_n$))          \\
                       \>        \>(append ($x_1$ $x_2$ \dots $x_n$) $ys$)) \\
                       \>($x_1$ $x_2$ \dots $x_n$))                         \\
\end{tabbing}
\end{quote}

We will prove that P(0) is true, and also that P($n+1$) is true whenever P($n$) is true. Then, we will cite mathematical induction to conclude that P($n$) is true, regardless of which natural number $n$ stands for.

\begin{quote}
\begin{tabbing}
P($n$) $\equiv$ (equal \=(prefix \=(len nil)          \\
                       \>        \>(append nil $ys$)) \\
                       \>nil)                         \\
\end{tabbing}
\end{quote}

As in the proof of the append suffix theorem, we start with the formula on one side of the equation and use known equations to gradually transform that formula to the one on the other side of the equation.

\begin{center}
\begin{tabular}{lll}
    & (prefix (len nil) (append nil $ys$))  &                                                      \\
$=$ & (prefix 0 (append nil $ys$))          & \{\emph{len0}\} (page \pageref{len-equations})   \\
$=$ & nil                                   & \{\emph{pfx0}\}                                      \\
\end{tabular}
\end{center}

That takes care of P(0). Next, we prove P($n+1$), assuming that P($n$) is true.

\begin{quote}
\begin{tabbing}
P($n+1$) $\equiv$ (equal \=(prefix \=(len ($x_1$ $x_2$ \dots $x_{n+1}$))        \\
                       \>        \>(append ($x_1$ $x_2$ \dots $x_{n+1}$) $ys$)) \\
                       \>($x_1$ $x_2$ \dots $x_{n+1}$))                         \\
\end{tabbing}
\end{quote}

\todo{Rex: This following indentation isn't perfect, but it's close.  I haven't figured out how to remove the vertical space before the tabbing, though I can probably hack it....}

\begin{center}
	\setlength{\topsep}{0pt}
	\setlength{\partopsep}{0pt}
\begin{tabular} {lp{3in}p{1.5in}}
    & \begin{tabbing}
			(prefix \=(len ($x_1$ $x_2$ \dots $x_{n+1}$)) \\
         	        \>(append ($x_1$ $x_2$ \dots $x_{n+1}$) $ys$))
		\end{tabbing}
	& \\
$=$ & \begin{tabbing}
		(prefix \=(len (cons $x_1$ ($x_2$ \dots $x_{n+1}$))) \\
                \>(append (cons $x_1$ ($x_2$ $x_2$ \dots $x_{n+1}$)) $ys$))
		\end{tabbing}
	& \{\emph{cons}\} (page \pageref{cons-axiom-informal}) \\
$=$ & \begin{tabbing}
			(prefix \=(+ (len ($x_2$ \dots $x_{n+1}$)) 1) \\
                    \>(cons $x_1$ (append ($x_2$ \dots $x_{n+1}$) $ys$)))
		\end{tabbing}
    & \{\emph{len1}\} (page \pageref{len-equations}) \hfill\break
      \{\emph{app1}\} (page \pageref{append-equations})    \\

$=$ & \begin{tabbing}
		(cons \=(first (cons $x_1$ ($x_2$ \dots $x_{n+1}$))) \\
			  \>(prefix \=(- (+ (len ($x_2$ \dots $x_{n+1}$)) 1) 1) \\
			  \>        \>(rest (cons $x_1$ (append ($x_2$ \dots $x_{n+1}$) $ys$)))))
		\end{tabbing}
	& \{\emph{pfx1}\} \\
$=$ & \begin{tabbing}
		(cons \=$x_1$ \\
			  \>(prefix \=(len ($x_2$ \dots $x_{n+1}$)) \\
			  \>        \>(append ($x_2$ \dots $x_{n+1}$) $ys$)))
		\end{tabbing}
	& \{\emph{first}\} (page \pageref{first-rest-cons}) \hfill\break
	  \{\emph{arithmetic}\} \hfill\break
	  \{\emph{rest}\} (page \pageref{first-rest-cons}) \\
$=$ & \begin{tabbing}
		(cons \=$x_1$ \\
			  \>($x_2$ \dots $x_{n+1}$) )
		\end{tabbing}
	& \{P($n$)\} \\
$=$ & ($x_1$ $x_2$ \dots $x_{n+1}$) & \{\emph{cons}\} (page \pageref{cons-axiom-informal}) \\
\end{tabular}
\end{center}

At this point we know three important facts about the append function:
\begin{quote}
\begin{itemize}
\item additive length theorem: (len (append $xs$ $ys$)) = (+ (len $xs$) (len $ys$))
\item append-prefix theorem: (prefix (len $xs$) (append $xs$ $ys$)) = $xs$
\item append-suffix theorem: (nthcdr (len $xs$) (append $xs$ $ys$)) = $ys$
\end{itemize}
\end{quote}

Together, these theorems provide a deep level of understanding of the append operation. 
They give us confidence that it correctly concatenates lists. 
We refer to these theorems as ``correctness properties'' for the append operation. 
There are, of course, an infinite variety of other facts about the append operation. 
Their relative importance depends on how we are using the operation.

A property that is sometimes important to know is that concatenation is ``associative''. 
That is, if there are three lists to be concatenated, 
you you could concatenate the first list with the concatenation of the last two. 
Or, you could concatenate the first two, then append the third list at the end.

\begin{center}
Theorem \{\emph{app-assoc}\} \\
\label{app-assoc}
(append $xs$ (append $ys$ $zs$)) = (append (append $xs$ $ys$) $zs$)
\end{center}

Addition and multiplication of numbers are also associative,
but subtraction and division aren't associative. 
Another way to say this is that the formula 
($\forall$$n$.A($n$)) is true, where the predicate A is defined as follows.

\begin{center}
\begin{tabular} {lll}
A($n$) $\equiv$  & (equal & (append ($x_1$ $x_2$ \dots $x_n$) (append $ys$ $zs$)) \\
                 &        & (append (append ($x_1$ $x_2$ \dots $x_n$) $ys$) $zs$) \\
\end{tabular}
\end{center}

Putting it this way makes the theorem amenable to a proof by mathematical induction. We leave that as a something you can use to practice your proof skills.

\todo{Ruben: The ExerciseList tag doesn't put in any vertical space, but should, I think.}

\begin{ExerciseList}
\Exercise Carry out a paper-and-pencil proof by mathematical induction of the \{\emph{app-assoc}\} theorem.

\begin{comment}
\todo{Rex: I don't understand your hint below.  Is it necessary? Ruben: Sorry. Thought they were necessary assumptions. Will omit them in the section that introduces proofs using mechanized logic.}

\Exercise State the \{\emph{app-assoc}\} theorem in ACL2 notation.
(\emph{Hint}. The theorem must be stated in the form of an implication whose hypothesis requires the objects $xs$ and $ys$ in the definition of A($n$) must be lists, so you will need to state the theorem as an implication whose hypothesis invokes the true-listp function twice to require $xs$ and $ys$ to be lists. The conclusion of the implication will, of course, be the equation between the two concatenation formulas that interchange the concatenation order.)

\Exercise Use Dracula to produce a mechanized proof of the \{\emph{app-assoc}\} theorem.
\end{comment}

\end{ExerciseList}

\todo{next section will introduce defthmd and proofs using the ACL2 mechanized logic by replaying all of the theorems of this section in ACL2 notation}

\section{Mechanized Logic}
\label{sec:mech-logic}
The proofs we have been doing depend on matching grammatical elements in formulas 
against templates in axioms and theorems. 
The formulas are then transformed to equivalent ones with different grammatical structures. 
Gradually, we move from a starting formula to a concluding one to verify an equation for a new theorem.

It is easy to make mistakes in this detailed, syntax-matching process, 
but computers carry it out flawlessly. 
This relieves us from an obligation to focus with monk-like devotion on the required grammatical analysis. 
We can leave it to the computer count on having it done right.

There are several mechanized logic systems that people use 
to assist with proofs of the kind we have been doing. 
One of them is ACL2 (A Computational Logic for Applicative Common Lisp). 
Theorems for the ACL2 proof engine are stated in the same form 
as properties for the doublecheck testing facility in Dracula. 
ACL2 has a built-in strategy for finding inductive proofs, 
and for some theorems it succeeds in fully automating proofs. 
It also permits people to guide it through proofs while it pushes through all of the grammatical details.

To illustrate how this works, we will go the theorems 
discussed earlier in this chapter, one by one. 
The notation for stating theorems in ACL2 form will be familiar, 
but not identical to the one we have been using 
for our paper-and-pencil proofs. 
For one thing, it employs prefix notation throughout, 
and we have been using a mixture of prefix and infix.

Our first proof by mathematical induction verified 
the additive law of concatenation (page \pageref{additive-law-concatenation}). 
Our statement of the theorem asserted that a proposition L($n$) is true, 
regardless of which natural number $n$ stands for: ($\forall$$n$.L($n$)).
 L($n$) is a shorthand for the following formula:
\begin{quote}
\begin{tabbing}
L($n$) $\equiv$ (= \=(len (append ($x_1$ $x_2$ \dots $x_n$) $ys$))  \\
                   \>(+ (len ($x_1$ $x_2$ \dots $x_n$)) (len $ys$)))
\end{tabbing}
\end{quote}

We could have used the doublecheck facility of Dracula to run tests on this property.

\begin{lstlisting}
(defproperty additive-law-of-concatenation-tst
    (xs :value (random-list-of (random-natural))
     ys :value (random-list-of (random-natural)))
  (= (len (append xs ys))
     (+ (len xs) (len ys))))
\end{lstlisting}

Of course, the doublecheck specification of this property 
cannot employ the informal notation of the numbered list interpretation 
(\pageref{numbered-list-interpretation}). 
Instead, the property simply uses a symbol $xs$ to stand for the list. 
The property does not state the length of $xs$, but we know 
its length will be some natural number $n$, so the property, as stated, 
has the same meaning as the formula that L($n$) stands for.

The statement of the additive law as a theorem in the form 
required by ACL2 cannot use the informal notation, either. 
In fact, the theorem takes a form that is like the property specification, 
except for the :value portion and the keyword ``defproperty''.

Theorem statements in ACL2 start with the ``defthmd'' keyword. 
After that comes a name for the theorem and the Boolean formula 
that expresses the meaning of the theorem, as illustrated in the following definition.

\begin{lstlisting}
(defthmd additive-law-of-concatenation-thm
  (= (len (append xs ys))
     (+ (len xs) (len ys))))
\end{lstlisting}

The mechanized logic of ACL2 fully automates the proof of this theorem. 
It uses a built-in, heuristic procedure to find an induction scheme 
and pushes the proof through on its own. 
To see ACL2 in action, enter the above theorem in the program pane of the Dracula window 
(the upper left pane), and press the start button in the ACL2 pane 
(the right half of the ACL2 window). 
When the Admit button lights up, press it.

After a short time, the theorem will turn green, 
indicating a successful proof. Details of the proof appear in the ACL2 pane. 
Later we will learn how to interpret some of these details, 
but for now, we are just looking for success 
(green coloring of the theorem in the program pane) or failure (red coloring).
Probably you can follow the above example to convert all theorems 
from this chapter into ACL2 theorem statements. 
Just to make sure, we will look at another one, 
then leave the rest for practice exercises.

The append-suffix theorem, which states that 
when the first argument in an append formula is a list of length $n$, 
then you can reconstruct the second argument by dropping $n$ elements 
from the front of the concatenation. 
We stated this this theorem in the form ($\forall$$n$.S($n$)), 
where S($n$) is a shorthand for the following formula.

\begin{center}
\begin{tabular}{lll}
S($n$) $\equiv$ (equal & (nthcdr & (len ($x_1$ $x_2$ \dots $x_n$))          \\
                       &         & (append ($x_1$ $x_2$ \dots $x_n$) $ys$)) \\
                       & $ys$)   &                                          \\
\end{tabular}
\end{center}

The following definition specifies this theorem in ACL2 notation.

\begin{lstlisting}
(defthmd append-suffix-thm
  (equal (nthcdr (len xs) (append xs ys))
         ys))
\end{lstlisting}

ACL2 can prove this theorem, but the proof requires knowing 
something about the algebra of numbers, such as the associative law of addition. 
Fortunately, someone has worked out a basic theory of numeric algebra in ACL2 terms, 
and we can take advantage of that by importing it into the working environment. 
To do this, we use a command called ``include-book''. 
The name of the book with the theory we need is ``arithmetic/top'', 
and it resides in the ``system'' directory of ACL2.

\begin{lstlisting}
(include-book "arithmetic/top" :dir :system)
\end{lstlisting}

The include-book command makes that theory accessible to the mechanized logic. When you put the command above the append-suffix theorem in the program pane and press the start button and then the Admit All button, ACL2 succeeds in the proof without further assistance. For practice, try it yourself.

\begin{ExerciseList}
\Exercise Define the \{\emph{append-prefix}\} theorem in ACL2 notation, and use Dracula to run it through the mechanized logic. If you state it correctly, the proof should succeed.

\Exercise Define the \{\emph{append associativity}\} theorem in ACL2 notation, and use Dracula to run it through the mechanized logic. If you state it correctly, the proof should succeed.
\end{ExerciseList}

%%% this might be enough on induction and lists for now ... move on to next chapter: binary numerals and the ripple-carry adder

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "book"
%%% End:
